{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a3_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-LGSqD7lOsP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjjGK8BplYYZ"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Running on GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Running on CPU\")\n",
        "    device = torch.device(\"cpu\")\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from itertools import product\n",
        "import time\n",
        "import random\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Global variables\n",
        "loadModelFromFile = False\n",
        "produceSubmissionFile = False\n",
        "modelFileName = './model.pth'\n",
        "\n",
        "# Image transformation composition\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "    \n",
        "])\n",
        "\n",
        "# Adapted dataset class to ease creationg of training, validation, and test sets\n",
        "class MyBetterDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.data = images\n",
        "        self.targets = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], int(self.targets[index])\n",
        "        img = Image.fromarray(img.astype('uint8'), mode='L')\n",
        "\n",
        "        if self.transform is not None:\n",
        "           img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# The convNN used for this project\n",
        "class MyConvNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyConvNN, self).__init__()\n",
        "\n",
        "        # SequentialLayer1\n",
        "        self.layer1 = nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Dropout(p=0.1))\n",
        "\n",
        "        # SequentialLayer2\n",
        "        self.layer2 = nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Dropout(p=0.1))\n",
        "\n",
        "        # SequentialLayer3\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Dropout(p=0.1))\n",
        "        \n",
        "        # SequentialLayer4\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(128, 256, kernel_size=5, stride=1, padding=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.Dropout(p=0.1))\n",
        "\n",
        "        # LinearLayer1\n",
        "        self.fc1 = nn.Linear(256 * 4 * 8, 1000, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "\n",
        "        # LinearLayer2\n",
        "        self.fc2 = nn.Linear(1000, 10, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    # How the network should do a forward pass\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Read image data and their labels\n",
        "allTrainData = pickle.load( open('gdrive/My Drive/ML-A3/Train.pkl', 'rb' ), encoding='bytes')\n",
        "allTrainLabels = np.genfromtxt('gdrive/My Drive/ML-A3/TrainLabels.csv', delimiter=',')\n",
        "\n",
        "# Zip images and labels together\n",
        "z = list(zip(allTrainData, allTrainLabels))\n",
        "\n",
        "# Shuffle data,label tuples\n",
        "random.shuffle(z)\n",
        "\n",
        "# Seperate data and labels back into seperate data structures\n",
        "allTrainData, allTrainLabels = zip(*z)\n",
        "\n",
        "# Define batchSize constant\n",
        "batchSize = 32 \n",
        "\n",
        "# If we're making a submission file, only use small a small portion of training data for validation\n",
        "# (a good model and good hyperparameters should already have been found)\n",
        "if produceSubmissionFile:\n",
        "    allTestData = pickle.load( open('gdrive/My Drive/ML-A3/Test.pkl', 'rb' ), encoding='bytes')\n",
        "    testData = MyBetterDataset(allTestData, allTrainLabels[:10000], img_transform)\n",
        "    testLoader = DataLoader(testData, batch_size=batchSize, shuffle=False)\n",
        "    trainData = MyBetterDataset(allTrainData[:55000], allTrainLabels[:55000], img_transform)\n",
        "    trainLoader = DataLoader(trainData, batch_size=batchSize, shuffle=True)\n",
        "    validationData = MyBetterDataset(allTrainData[55000:], allTrainLabels[55000:], img_transform)\n",
        "    validationLoader  = DataLoader(validationData, batch_size=batchSize, shuffle=True)\n",
        "else:\n",
        "    trainData = MyBetterDataset(allTrainData[10000:], allTrainLabels[10000:], img_transform)\n",
        "    validationData = MyBetterDataset(allTrainData[5000:10000], allTrainLabels[5000:10000], img_transform)\n",
        "    testData = MyBetterDataset(allTrainData[:5000],allTrainLabels[:5000], img_transform)\n",
        "\n",
        "    trainLoader = DataLoader(trainData, batch_size=batchSize, shuffle=True)\n",
        "    validationLoader  = DataLoader(validationData, batch_size=batchSize, shuffle=True)\n",
        "    testLoader  = DataLoader(testData, batch_size=batchSize, shuffle=False)\n",
        "\n",
        "# Label names\n",
        "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Initialize NN and load the saved one from file if applicable\n",
        "net = MyConvNN()\n",
        "if loadModelFromFile:\n",
        "    net.load_state_dict(torch.load(modelFileName))\n",
        "net.cuda()\n",
        "\n",
        "# Set loss and optimizer functions\n",
        "criterion = nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.00035, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
        "\n",
        "# Number of epoch the NN will train for\n",
        "num_epochs = 1\n",
        "\n",
        "print(\"Start training\")\n",
        "print(\"Batch size:\", batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    startTime = time.time()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainLoader):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Some optimizers/criterions need things to be in the one-hot format\n",
        "        # Use \"onehots\" in liu of \"labels\"\n",
        "        onehots = np_utils.to_categorical(labels.cpu(), 10)\n",
        "        onehots = torch.from_numpy(onehots).cuda()\n",
        "\n",
        "        # Set the parameter gradients to 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Feed forward\n",
        "        outputs = net(inputs.cuda())\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backwards\n",
        "        loss.backward()\n",
        "\n",
        "        # Run optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Show some (potentially wildly inaccurate) info\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(\"[\", epoch + 1, \",\", i + 1, \"] loss:\", running_loss / 200)\n",
        "            running_loss = 0.0\n",
        "        \n",
        "    # End of epoch info\n",
        "    print(\"End of Epoch\", epoch + 1, \". Epoch time:\", time.time() - startTime, \"Loss:\", loss.item())\n",
        "\n",
        "    # Show acc on validation set at end of epoch\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in validationLoader:\n",
        "            images, labels = data\n",
        "            outputs = net(images.cuda())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print('Accuracy of the network on the 10000 validation images: %d %%' % (100 * correct / total))\n",
        "    \n",
        "    # Save the model after each epoch in case of interuption\n",
        "    torch.save(net.state_dict(), modelFileName)\n",
        "print('Training completed')\n",
        "\n",
        "#Predict test set\n",
        "correct = 0\n",
        "total = 0\n",
        "preds = []\n",
        "timeO = time.time()\n",
        "with torch.no_grad():\n",
        "    for data in testLoader:\n",
        "        images, labels = data\n",
        "        outputs = net(images.cuda())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        if not produceSubmissionFile:\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        for p in predicted:\n",
        "            preds.append(p)\n",
        "print(\"Time to classify: \", time.time() - timeO)\n",
        "\n",
        "if not produceSubmissionFile:\n",
        "    # Print out test set accuracy if we're not making a file for submission\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    # Sanity check\n",
        "    rightOnes = 0\n",
        "    for i in range(len(preds)):\n",
        "        if preds[i] == testData.targets[i]:\n",
        "            rightOnes += 1\n",
        "    print(rightOnes / len(preds))\n",
        "\n",
        "else:\n",
        "    # Write predictions to file if we're making a submission file\n",
        "    from google.colab import files\n",
        "    with open('submission.txt', 'w') as f:\n",
        "        f.write('id,output\\n')\n",
        "        for x in range(len(preds)):\n",
        "            f.write(str(x) + \",\" + str(preds[x].item()) + \"\\n\")\n",
        "    print(\"Predictions completed and saved to file 'submission.txt'.\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}